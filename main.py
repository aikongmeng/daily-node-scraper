import requests
import re
import time
from urllib.parse import urljoin, urlparse
from datetime import datetime

# ----------------é…ç½®åŒºåŸŸ----------------
# å¾…æŠ“å–çš„ç½‘ç«™åˆ—è¡¨
URLS = [
    "https://www.freeclashnode.com/",
    "https://yoyapai.com/mianfeijiedian",
    "https://wanzhuanmi.com/",
    "https://oneclash.cc/",
    "https://clashnodes.com/",
    "https://clashnode.cc/",
    "https://www.mibei77.com/",
    "https://www.cfmem.com/",
    "https://www.85la.com/",
    "https://nodecats.com/",
    "https://github.com/Pawdroid/Free-servers" # GitHub è¿™ç§é¡µé¢å¯èƒ½ç›´æ¥æœ‰é“¾æ¥ï¼Œä¹Ÿå¯èƒ½éœ€è¦æ·±æŒ–
]

# æ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚å¤´
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# æ¯ä¸ªç½‘ç«™æœ€å¤šæ·±å…¥è®¿é—®å¤šå°‘ä¸ªå­é¡µé¢ï¼ˆé˜²æ­¢è¶…æ—¶ï¼‰
MAX_DEPTH_PAGES = 15 

# ----------------æ ¸å¿ƒä»£ç ----------------

def get_html(url):
    """å‘é€è¯·æ±‚è·å–ç½‘é¡µæºç """
    try:
        response = requests.get(url, headers=HEADERS, timeout=10)
        response.encoding = 'utf-8' # å¼ºåˆ¶UTF-8ï¼Œé˜²æ­¢ä¸­æ–‡ä¹±ç 
        return response.text
    except Exception as e:
        print(f"    [Error] Failed to fetch {url}: {e}")
        return None

def extract_subs(content):
    """ä»æ–‡æœ¬å†…å®¹ä¸­æ­£åˆ™åŒ¹é… .yaml æˆ– .txt çš„ http é“¾æ¥"""
    if not content:
        return []
    # æ­£åˆ™è¯´æ˜ï¼šåŒ¹é… http/s å¼€å¤´ï¼Œä¸åŒ…å«å¼•å·/ç©ºæ ¼/å°–æ‹¬å·ï¼Œä»¥ .yaml/.yml/.txt ç»“å°¾
    pattern = r'https?://[^\s<>"\'\(\)]+?(?:\.yaml|\.yml|\.txt)'
    links = re.findall(pattern, content)
    return links

def extract_internal_links(base_url, html):
    """æå–é¦–é¡µä¸­çš„å†…éƒ¨é“¾æ¥ï¼ˆå¯èƒ½æ˜¯æ–‡ç« è¯¦æƒ…é¡µï¼‰"""
    if not html:
        return []
    
    # æå–æ‰€æœ‰ href
    raw_links = re.findall(r'href=["\'](.*?)["\']', html)
    
    valid_links = []
    domain = urlparse(base_url).netloc
    
    for link in raw_links:
        # è¡¥å…¨ç›¸å¯¹è·¯å¾„
        full_link = urljoin(base_url, link)
        parsed = urlparse(full_link)
        
        # è¿‡æ»¤é€»è¾‘ï¼š
        # 1. å¿…é¡»æ˜¯åŒåŸŸå
        # 2. æ’é™¤ .css, .js, .png ç­‰éé¡µé¢èµ„æº
        # 3. æ’é™¤ /tag/, /category/ ç­‰åˆ†ç±»é¡µï¼Œå°½é‡åªæŠ“æ–‡ç« é¡µ
        # 4. æ’é™¤ #é”šç‚¹
        if parsed.netloc == domain and "#" not in full_link:
            if not re.search(r'\.(css|js|png|jpg|jpeg|gif|ico|xml|json)$', parsed.path, re.I):
                # ç®€å•çš„å»é‡åˆ—è¡¨
                if full_link not in valid_links and full_link != base_url:
                     # é’ˆå¯¹åšå®¢ç±»ç½‘ç«™ï¼Œé€šå¸¸æ–‡ç« é“¾æ¥æ¯”è¾ƒé•¿ï¼Œæˆ–è€…åŒ…å«æ•°å­—/æ—¥æœŸ
                     # è¿™é‡Œåšä¸€ä¸ªç®€å•çš„é•¿åº¦åˆ¤æ–­ï¼Œè¿‡æ»¤æ‰ overly short links (like /, /about)
                     if len(parsed.path) > 4: 
                        valid_links.append(full_link)
    
    return valid_links

def main():
    all_subs = set()
    
    print(f"ğŸš€ Task started at {datetime.now()}\n")

    for site_url in URLS:
        print(f"ğŸŒ Scanning Site: {site_url}")
        
        # 1. è®¿é—®é¦–é¡µ
        home_html = get_html(site_url)
        if not home_html:
            continue
            
        # 2. å°è¯•ç›´æ¥åœ¨é¦–é¡µæ‰¾è®¢é˜…é“¾æ¥
        home_subs = extract_subs(home_html)
        if home_subs:
            print(f"    [Success] Found {len(home_subs)} subs on Homepage.")
            for sub in home_subs:
                all_subs.add(sub)
        
        # 3. æŒ–æ˜äºŒçº§é¡µé¢ (Deep Dive)
        # æå–é¦–é¡µçš„æ‰€æœ‰é“¾æ¥ï¼Œé€‰å–å‰ MAX_DEPTH_PAGES ä¸ªè¿›è¡Œè®¿é—®
        internal_links = extract_internal_links(site_url, home_html)
        
        # åªè¦å‰å‡ ä¸ªï¼Œå› ä¸ºé€šå¸¸æœ€æ–°çš„èŠ‚ç‚¹æ–‡ç« åœ¨æœ€ä¸Šé¢
        target_links = internal_links[:MAX_DEPTH_PAGES]
        
        if target_links:
            print(f"    [Deep Dive] Visiting top {len(target_links)} sub-pages...")
            
            for sub_page_url in target_links:
                # å»¶æ—¶ä¸€ä¸‹ï¼Œå¯¹æœåŠ¡å™¨å‹å¥½
                time.sleep(1) 
                sub_html = get_html(sub_page_url)
                deep_subs = extract_subs(sub_html)
                
                if deep_subs:
                    print(f"      -> Found {len(deep_subs)} subs in {sub_page_url}")
                    for sub in deep_subs:
                        all_subs.add(sub)
        else:
             print("    [Info] No relevant sub-pages found.")

        print("-" * 30)

    # 4. ä¿å­˜ç»“æœ
    save_to_file(all_subs)

def save_to_file(links):
    filename = "nodes_list.txt"
    # è¿‡æ»¤ä¸€äº›åƒåœ¾é“¾æ¥ï¼ˆå¦‚åŒ…å« localhost, example ç­‰ï¼‰
    valid_links = [l for l in links if "localhost" not in l and "127.0.0.1" not in l]
    
    with open(filename, "w", encoding="utf-8") as f:
        f.write(f"# Auto-scraped Node Subscriptions\n")
        f.write(f"# Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"# Total found: {len(valid_links)}\n\n")
        for link in sorted(valid_links):
            f.write(link + "\n")
    
    print(f"\nâœ… Done! Saved {len(valid_links)} unique links to {filename}")

if __name__ == "__main__":
    main()
